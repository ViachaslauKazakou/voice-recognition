import requests
from ollama import chat
from ollama import ChatResponse
from langchain.chains import LLMChain, ConversationChain
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from src.utils import timer, setup_logger
from src.simple_rag_manager import SimpleRAG
from src.rag_langchain import AdvancedRAG
from langchain.chains import RetrievalQA

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
logger = setup_logger(__name__)


class Role:
    """
    Class to manage roles for AI interactions.
    """

    senior_python_engineer: str = "You are a Senior software engineer with main skill Python, answer the next question:"
    senior_frontend_engineer: str = "You are a Senior frontend engineer with main skill JavaScript, answer the next question:"
    senior_backend_engineer: str = "You are a Senior backend engineer with main skill Python, answer the next question:"
    senior_data_scientist: str = "You are a Senior data scientist with main skill Python, answer the next question:"
    forum_moderator: str = "You are a forum moderator, answer the next question:"
    forum_troll: str = "You are a forum talkative troll names Alaev, answer the next question:"


class AIModels:
    """
    Class to manage AI models and their identifiers.
    """

    light: str = "google/flan-t5-base"
    medium: str = "google/flan-t5-large"
    heavy: str = "google/flan-t5-xl"
    light_1: str = "mistralai/Mistral-7B-Instruct-v0.2"
    distilgpt2: str = "distilgpt2"
    llama_mistral: str = "mistral:latest"
    deepseek: str = "deepseek-r1"
    gemma: str = "gemma3:latest"
    # heavy: str = "mistralai/Mistral-7B-Instruct-v0.2"
    # light: str = "meta-llama/Llama-2-7b-chat-hf"
    # medium: str = "meta-llama/Llama-2-7b-chat-hf"


class AiManager:
    """
    Class to manage AI models and their interactions.
    """

    def __init__(self):
        # Initialize Ollama with LangChain
        self.model = AIModels.gemma
        self.llm = Ollama(model=self.model)
        self.memory = ConversationBufferMemory()
        # Create a conversation chain
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞
        self.logger = setup_logger(f"{__name__}.{self.__class__.__name__}")

        self.logger.info("AiManager initialized successfully")

    def ask_ollama_api(self, prompt):
        """
        Ask a question to the Ollama API and return the response. Using internal api for request"""
        modified_prompt = (
            f"You are a Senior software engineer with main skill Python, answer the next question: {prompt}"
        )
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "mistral", "prompt": modified_prompt, "stream": False},  # –∏–ª–∏ llama2, gemma –∏ —Ç.–ø.
        )
        print(response.json()["response"])
        return response.json()["response"]

    @timer
    def ask_ollama(self, prompt, translate=True):
        self.logger.info(f"Simple Asking Ollama: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")

        role = "You are a Senior software engineer with main skill Python, answer the next question:"
        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt = f"{prompt} (–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"

        modified_prompt = f"[INST]{role} {prompt}[/INST]"

        try:
            response: ChatResponse = chat(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": modified_prompt,
                    },
                ],
            )

            result = response["message"]["content"]
            self.logger.info(f"Response received: {len(result)} characters")
            return result

        except Exception as e:
            self.logger.error(f"Error in ask_ollama: {str(e)}")
            raise

    @timer
    def ask_ollama_memory(self, prompt, translate=True) -> str:
        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt += " (–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"

        # Use the conversation chain which maintains history
        response = self.conversation.predict(input=prompt)
        print(response)
        return response

    def question_chain(self, initial_prompt, follow_ups=2, translate=True):
        """
        Create a chain of questions where each follow-up is based on previous answers.

        Args:
            initial_prompt: The starting question
            follow_ups: Number of follow-up questions to ask
            translate: Whether to translate responses to Russian

        Returns:
            List of question-answer pairs
        """
        qa_chain = []

        # Ask initial question
        current_prompt = initial_prompt
        response = self.ask_ollama_memory(current_prompt, translate)
        qa_chain.append({"question": current_prompt, "answer": response})

        # Generate follow-up questions
        for i in range(follow_ups):
            # Create a follow-up question based on previous answer
            response_preview = response['result'][:100] if len(response) > 100 else response
            follow_up_prompt = (
                f"Based on your previous answer that '{response_preview}...', can "
                "you elaborate further on the most important aspect mentioned?"
            )

            # Get response to follow-up
            response = self.ask_ollama_memory(follow_up_prompt, translate)
            qa_chain.append({"question": follow_up_prompt, "answer": response})

        return qa_chain

    def guided_learning_chain(self, topic, depth=3, translate=True):
        """
        Create a sequential learning experience that builds understanding step by step.

        Args:
            topic: The topic to learn about
            depth: How many levels to explore
            translate: Whether to translate responses to Russian

        Returns:
            Dictionary with the learning progression
        """
        # Initial broad question about the topic
        initial_q = f"What is {topic} and why is it important?"
        intro = self.ask_ollama_memory(initial_q, translate)

        # Ask for core concepts
        concepts_q = f"What are the 3 most important concepts in {topic} that I should understand first?"
        concepts = self.ask_ollama_memory(concepts_q, translate)

        # Extract concepts and explore each one
        detailed_explanations = []

        # For each level of depth, ask increasingly specific questions
        for level in range(depth):
            depth_q = (
                f"For someone at a {level+1}/5 understanding level of {topic}, what should they focus on learning next?"
            )
            explanation = self.ask_ollama_memory(depth_q, translate)
            detailed_explanations.append({"level": level + 1, "focus": explanation})

        # Final practical application
        apply_q = f"Give me a practical example or exercise to apply what I've learned about {topic}"
        application = self.ask_ollama_memory(apply_q, translate)

        return {
            "topic": topic,
            "introduction": intro,
            "core_concepts": concepts,
            "learning_path": detailed_explanations,
            "practical_application": application,
        }

    def decision_tree_chain(self, problem_statement, max_depth=3, translate=True):
        """
        Create a decision tree by asking follow-up questions based on previous answers.

        Args:
            problem_statement: The initial problem to solve
            max_depth: Maximum depth of the decision tree
            translate: Whether to translate responses to Russian

        Returns:
            A dictionary representing the decision tree
        """

        def explore_branch(question, current_depth=0):
            if current_depth >= max_depth:
                return {"question": question, "answer": self.ask_ollama_memory(question, translate)}

            response = self.ask_ollama_memory(question, translate)

            # Generate two alternative paths to explore
            options_q = (
                f"Based on the answer '{response['result'][:100]}...', what are two different "
                "approaches or perspectives we could explore next?"
            )
            options = self.ask_ollama_memory(options_q, translate)

            # Generate specific questions for each path
            path_q = "Based on these options, formulate two specific questions that would explore each path."
            path_questions_text = self.ask_ollama_memory(path_q, translate)

            # For simplicity, we'll assume the model returns reasonably formatted questions
            # In production, you might want to use more structured prompting
            path1_q = f"Let's explore the first approach: {
                path_questions_text.split('1.')[1].split('2.')[0] if '1.' in path_questions_text else 'Tell me more about the first approach'}"
            path2_q = f"Let's explore the second approach: {
                path_questions_text.split('2.')[1] if '2.' in path_questions_text else 'Tell me more about the second approach'}"
            # Recursively explore both paths

            return {
                "question": question,
                "answer": response,
                "paths": [explore_branch(path1_q, current_depth + 1), explore_branch(path2_q, current_depth + 1)],
            }

        # Start with the problem statement
        return explore_branch(problem_statement)

    @timer
    def ask_ollama_with_rag(self, prompt, translate=True, use_rag=True):
        """
        Ask Ollama with RAG support
        """
        logger.info(f"==== Processing prompt: {prompt} with simple RAG: {use_rag}")
        self.rag_simple = SimpleRAG("knowledge_base")  # Ensure RAG is initialized
        context = ""

        if use_rag:
            # Retrieve relevant documents
            relevant_docs = self.rag_simple.search(prompt, top_k=3)

            if relevant_docs:
                logger.info(f"Found {len(relevant_docs)} relevant documents for RAG.")
                context = "\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n"
                for i, doc in enumerate(relevant_docs, 1):
                    context += f"{i}. {doc}\n"
                context += "\n"

        # Prepare the prompt
        role = "You are a Senior software engineer with main skill Python. Use the provided context to answer the question accurately."

        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt = f"{prompt} (–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"

        # Combine context and prompt
        full_prompt = f"[INST]{role}\n{context}–í–æ–ø—Ä–æ—Å: {prompt}[/INST]"

        response: ChatResponse = chat(
            model=self.model,
            messages=[
                {
                    "role": "user",
                    "content": full_prompt,
                },
            ],
        )

        print(response["message"]["content"])
        return response.message.content

    @timer
    def ask_ollama_memory_with_rag(self, prompt, translate=True, use_rag=True):
        """
        Ask Ollama with both memory and RAG
        """
        context = ""

        if use_rag:
            relevant_docs = self.rag_simple.search(prompt, top_k=3)

            if relevant_docs:
                context = "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n"
                for i, doc in enumerate(relevant_docs, 1):
                    context += f"{i}. {doc}\n"
                context += "\n"

        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt += " (–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"

        # Add context to the prompt
        full_prompt = f"{context}–í–æ–ø—Ä–æ—Å: {prompt}"

        # Use conversation chain with enhanced prompt
        response = self.conversation.predict(input=full_prompt)
        print(response)
        return response

    @timer
    def ask_ollama_langchain_rag(self, prompt, translate=True):
        """
        Ask Ollama using LangChain RAG
        """
        logger.info(f" ===== Processing prompt: {prompt} with LangChain RAG")
        self.rag = AdvancedRAG("knowledge_base")
        # Create RAG chain with LangChain
        if self.rag.retriever:
            logger.info("Creating RAG chain with LangChain")
            self.rag_chain = RetrievalQA.from_chain_type(
                llm=self.llm, 
                chain_type="stuff", 
                retriever=self.rag.retriever, 
                return_source_documents=True
            )
        else:
            logger.warning("RAG retriever is not initialized. Using fallback method.")
            self.rag_chain = None
        if not hasattr(self, "rag_chain") or not self.rag_chain:
            # Fallback to regular ask_ollama
            return self.ask_ollama(prompt, translate)

        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt += " (–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"

        try:
            # Use RAG chain
            response = self.rag_chain.invoke({"query": prompt})

            answer = response["result"]
            sources = response.get("source_documents", [])

            print(f"Answer: {answer}")
            if sources:
                print(f"Sources: {len(sources)} documents used")

            return answer

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ RAG: {e}")
            # Fallback to regular method
            return self.ask_ollama(prompt, translate)

    @timer
    def ask_ollama_langchain_rag_enhanced(self, prompt, translate=True, rag_method="adaptive", role=None):
        """
        Ask Ollama using enhanced LangChain RAG with improved ranking
        """
        logger.info(f"==== Processing prompt with enhanced RAG using method: {rag_method}")
        logger.info(f"Processing prompt with enhanced RAG: {prompt[:100]}...")
        
        self.rag = AdvancedRAG("knowledge_base")
        
        if not self.rag.retriever:
            logger.warning("RAG retriever is not initialized. Using fallback method.")
            return self.ask_ollama(prompt, translate)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        relevant_docs = self.rag.get_relevant_docs(prompt, method=rag_method)
        
        if not relevant_docs:
            logger.warning("No relevant documents found. Using fallback method.")
            return self.ask_ollama(prompt, translate)
        logger.info(f"Found {len(relevant_docs)} relevant documents using {rag_method} method")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞—á–µ—Å—Ç–≤–µ
        logger.info("Preparing context for response")
        context = "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏):\n"
        for i, doc in enumerate(relevant_docs, 1):
            context += f"{i}. {doc}\n"
        context += "\n"
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç
        logger.info("Preparing full prompt for Ollama")
        role = role or "You are a Senior software engineer with main skill Python. Use the provided context to answer the question accurately."
        # role = "You are a Senior software engineer. Use the provided context (ranked by relevance) to answer accurately."
        
        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt = f"{prompt} (–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"
        
        full_prompt = f"[INST]{role}\n{context}–í–æ–ø—Ä–æ—Å: {prompt}[/INST]"
        
        try:
            response: ChatResponse = chat(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": full_prompt,
                    },
                ],
            )
            
            answer = response["message"]["content"]
            logger.info(f"Enhanced RAG response generated using {len(relevant_docs)} documents")
            return answer
            
        except Exception as e:
            logger.error(f"Error in enhanced RAG: {e}")
            return self.ask_ollama(prompt, translate)
        
    def enhanced_adaptig_rag(self, prompt=None, translate=True, compare_methods=None, analyze_quality=False):
        """
        Ask Ollama using an enhanced RAG chain with improved ranking.
        """
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ RAG
        test_prompt = prompt if prompt else input("–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è RAG: ")

        print("\nüîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ RAG:")
        print("-" * 50)
        if not compare_methods:
            methods = ["adaptive", "filtered", "ranked", "contextual"]
            for method in methods:
                print(f"\nüìä –ú–µ—Ç–æ–¥: {method}")
                try:
                    response = ai_manager.ask_ollama_langchain_rag_enhanced(
                        test_prompt, 
                        translate=translate, 
                        rag_method=method
                    )
                    print(f"–û—Ç–≤–µ—Ç: {response['result'][:500]}...")
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞: {e}")
        else:
            if compare_methods not in ["adaptive", "filtered", "ranked", "contextual"]:
                print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –º–µ—Ç–æ–¥: {compare_methods}. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã: adaptive, filtered, ranked, contextual")
                return
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
            print(f"\nüìä –ú–µ—Ç–æ–¥: {compare_methods}")
            try:
                response = self.ask_ollama_langchain_rag_enhanced(
                    test_prompt, 
                    translate=translate, 
                    rag_method=compare_methods
                )
                print(f"–û—Ç–≤–µ—Ç: {response['result'][:500]}...")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞: {e}")
        if analyze_quality:
            self.analyze_docs_quality(test_prompt)

    def analyze_docs_quality(self, prompt):
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print("\nüìà –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
        rag = ai_manager.rag
        docs_info = rag.get_ranked_relevant_docs(prompt)
        
        for i, doc in enumerate(docs_info):
            print(f"{i+1}. Score: {doc['similarity_score']:.4f}, Source: {doc['source']}")
            print(f"   Preview: {doc['content'][:100]}...")
        


# –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É knowledge_base –∏ –¥–æ–±–∞–≤—å—Ç–µ —Ç—É–¥–∞ .txt —Ñ–∞–π–ª—ã
# –ù–∞–ø—Ä–∏–º–µ—Ä: knowledge_base/python_basics.txt, knowledge_base/algorithms.txt

if __name__ == "__main__":
    print("=" * 100)
    print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI Manager...")
    ai_manager = AiManager()

    print("\n" + "=" * 100)
    print("ü§ñ –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø AI MANAGER".center(100))
    print("=" * 100)

    # 1. –û–±—ã—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å –±–µ–∑ RAG
    print("=" * 100)
    print("\nüìù 1. –û–ë–´–ß–ù–´–ô –í–û–ü–†–û–° (–±–µ–∑ RAG)")
    
    # response1 = ai_manager.ask_ollama("–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä –≤ Python?")
    # print(f"–û—Ç–≤–µ—Ç: {response1['result'][:200]}...")
    print("-" * 120)
    # 2. –í–æ–ø—Ä–æ—Å —Å Simple RAG
    print("=" * 100)
    print("\nüîç 2. –í–û–ü–†–û–° –° SIMPLE RAG")
   
    # response2 = ai_manager.ask_ollama_with_rag(
    #     "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä –≤ Python? –ü—Ä–∏–≤–µ–¥–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä.", use_rag=True
    # )
    # print(f"–û—Ç–≤–µ—Ç —Å RAG: {response2}")
    print("-" * 120)

    # 3. –í–æ–ø—Ä–æ—Å —Å LangChain RAG
    print("=" * 100)
    print("üß† 3. –í–û–ü–†–û–° –° LANGCHAIN RAG".center(120))
    # response3 = ai_manager.ask_ollama_langchain_rag(
    #     "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä –≤ Python? –ü—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.", translate=True
    # )
    # print(f"–û—Ç–≤–µ—Ç —Å LangChain RAG: {response3['result'][:100]}...")
    print("-" * 100)

    # 4. –¶–µ–ø–æ—á–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ (Question Chain)
    print("\n" + "=" * 100)
    print("\nüîó 4. –¶–ï–ü–û–ß–ö–ê –í–û–ü–†–û–°–û–í")
    # qa_chain = ai_manager.question_chain("–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã –û–û–ü –≤ Python", follow_ups=2, translate=True)
    # print(f"–°–æ–∑–¥–∞–Ω–æ {len(qa_chain)} –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ü–µ–ø–æ—á–∫–µ:")
    # for i, qa in enumerate(qa_chain, 1):
    #     print(f"  {i}. –í–æ–ø—Ä–æ—Å: {qa['question']}...")
    #     print(f"     –û—Ç–≤–µ—Ç: {qa['answer']}...")
    print("-" * 100)

    # 5. –£–ø—Ä–∞–≤–ª—è–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Guided Learning)
    print("=" * 100)
    print("\nüéì 5. –£–ü–†–ê–í–õ–Ø–ï–ú–û–ï –û–ë–£–ß–ï–ù–ò–ï")
    print("=" * 100)  
    # learning_path = ai_manager.guided_learning_chain("–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Python", depth=2, translate=True)
    # print(f"–¢–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è: {learning_path['topic']}")
    # print(f"–í–≤–µ–¥–µ–Ω–∏–µ: {learning_path['introduction'][:100]}...")
    # print(f"–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: {learning_path['core_concepts']}...")
    # print(f"–®–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {len(learning_path['learning_path'])}")
    print("-" * 100)
 
    # 6. –î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π (Decision Tree)
    print("  ")
    print("=" * 100)
    print("üå≥ 6. –î–ï–†–ï–í–û –†–ï–®–ï–ù–ò–ô".center(100))
    print("=" * 100)
    # decision_tree = ai_manager.decision_tree_chain(
    #     "–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –≤ Python?", max_depth=2, translate=True
    # )
    # print(f"–ü—Ä–æ–±–ª–µ–º–∞: {decision_tree['question'][:60]}...")
    # print(f"–û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç: {decision_tree['answer'][:100]}...")
    # if "paths" in decision_tree:
    #     print(f"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –ø—É—Ç–µ–π: {len(decision_tree['paths'])}")

    # 7. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —Å –ø–∞–º—è—Ç—å—é
    print("  ")
    print("=" * 100)
    print("üß† 7. –°–†–ê–í–ù–ï–ù–ò–ï: –û–ë–´–ß–ù–´–ô vs –° –ü–ê–ú–Ø–¢–¨–Æ".center(100))
    print("=" * 100)

    # –û–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
    # regular_response = ai_manager.ask_ollama("–ö–∞–∫–∏–µ –µ—Å—Ç—å —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ Python?", translate=True)
    # print(f"–û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç: {regular_response['message'][:100]}...")

    # –ú–µ—Ç–æ–¥ —Å –ø–∞–º—è—Ç—å—é
    # memory_response = ai_manager.ask_ollama_memory(
    #     "–ê –∫–∞–∫–æ–π –∏–∑ —ç—Ç–∏—Ö —Ç–∏–ø–æ–≤ –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π?", translate=True
    # )
    # print(f"–û—Ç–≤–µ—Ç —Å –ø–∞–º—è—Ç—å—é: {memory_response['message'][:100]}...")

    # 8. –†–∞–±–æ—Ç–∞ —Å RAG –∏ –ø–∞–º—è—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    print("  ")
    print("=" * 100)
    print("\nüîÑ 8. RAG + –ü–ê–ú–Ø–¢–¨")

    # rag_memory_response = ai_manager.ask_ollama_memory_with_rag(
    #     "–ü–æ–∫–∞–∂–∏ –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è set –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤", translate=True, use_rag=True
    # )
    # print(f"RAG + –ü–∞–º—è—Ç—å: {rag_memory_response[:100]}...")
    print("-" * 100)

    # 9. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    print("  ")
    print("=" * 100)
    print("\nüíæ 9. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–ï–®–ò–†–û–í–ê–ù–ò–ò")
    print("=" * 100)
    try:
        # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–µ—à–µ RAG
        if hasattr(ai_manager, "rag_simple") and ai_manager.rag_simple:
            cache_info = ai_manager.rag_simple.get_cache_info()
            print(f"Simple RAG –∫–µ—à: {cache_info}")

        if hasattr(ai_manager, "rag") and ai_manager.rag:
            advanced_cache_info = ai_manager.rag.get_cache_info()
            print(f"Advanced RAG –∫–µ—à: {advanced_cache_info}")
        else:
            print("Advanced RAG –∫–µ—à –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")

    except Exception as e:
        print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–µ—à–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")

    # 10. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä: –ü–æ–º–æ—â–Ω–∏–∫ –ø–æ –∫–æ–¥—É
    print("  ")
    print("=" * 100)
    print("\nüë®‚Äçüíª 10. –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ô –ü–†–ò–ú–ï–†: –ü–û–ú–û–©–ù–ò–ö –ü–û –ö–û–î–£")
    print("=" * 100)
    # code_question = """
    # –£ –º–µ–Ω—è –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª [1, 2, 2, 3, 3, 3, 4]. 
    # –ö–∞–∫ –Ω–∞–π—Ç–∏ —á–∏—Å–ª–æ, –∫–æ—Ç–æ—Ä–æ–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —á–∞—â–µ –≤—Å–µ–≥–æ? 
    # –ü–æ–∫–∞–∂–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ —Ä–µ—à–µ–Ω–∏—è.
    # """

    # code_response = ai_manager.ask_ollama_with_rag(code_question, translate=True, use_rag=True)
    # print(f"–†–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏: {code_response['result'][:200]}...")
    print("-" * 100)

    # 10.1 –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä: –ü–æ–º–æ—â–Ω–∏–∫ –ø–æ –∫–æ–¥—É
    print("  ")
    print("=" * 100)
    print("\nüë®‚Äçüíª 10.1 –†–ê–°–®–ò–†–ï–ù–ù–´–ô RAG: –ö–≠–® –ò –†–ï–†–ê–ù–ö–ò–ù–ì")
    print("=" * 100)
    prompt = "–ö—Ç–æ —Ç–∞–∫–æ–π –í–ª–∞–¥–∏–º–∏—Ä –õ–µ–Ω–∏–Ω? –†–∞—Å—Å–∫–∞–∂–∏ –æ –µ–≥–æ –≤–∫–ª–∞–¥–µ –≤ –∏—Å—Ç–æ—Ä–∏—é –†–æ—Å—Å–∏–∏ –∏ –º–∏—Ä–∞. –ü—Ä–∏–≤–µ–¥–∏ –∫–ª—é—á–µ–≤—ã–µ –¥–∞—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è."
    print(f"–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {prompt}")
    ai_manager.enhanced_adaptig_rag(prompt, translate=True, compare_methods=None, analyze_quality=True)
    print("-" * 100)


    # 11. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    print("  ")
    print("=" * 100)
    print("\nüéÆ 11. –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú")
    print("-" * 50)
    print("–î–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π –±–ª–æ–∫:")

    """
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º - —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    while True:
        user_input = input("\n‚ùì –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ")
        if user_input.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥']:
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        print("\n–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥:")
        print("1 - –û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç")
        print("2 - –° Simple RAG")
        print("3 - –° LangChain RAG")
        print("4 - –° –ø–∞–º—è—Ç—å—é")
        
        choice = input("–í–∞—à –≤—ã–±–æ—Ä (1-4): ")
        
        try:
            if choice == "1":
                response = ai_manager.ask_ollama(user_input, translate=True)
            elif choice == "2":
                response = ai_manager.ask_ollama_with_rag(user_input, translate=True, use_rag=True)
            elif choice == "3":
                response = ai_manager.ask_ollama_langchain_rag(user_input, translate=True)
            elif choice == "4":
                response = ai_manager.ask_ollama_memory(user_input, translate=True)
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
                continue
                
            print(f"\nü§ñ –û—Ç–≤–µ—Ç: {response}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    """

    print("\n‚úÖ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 80)

    # 12. –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°–í–û–î–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
    print("-" * 50)
    print("üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã:")
    print("  ‚Ä¢ ask_ollama() - –±–∞–∑–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã")
    print("  ‚Ä¢ ask_ollama_with_rag() - —Å Simple RAG")
    print("  ‚Ä¢ ask_ollama_langchain_rag() - —Å LangChain RAG")
    print("  ‚Ä¢ ask_ollama_memory() - —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –ø–∞–º—è—Ç—å—é")
    print("  ‚Ä¢ question_chain() - —Ü–µ–ø–æ—á–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤")
    print("  ‚Ä¢ guided_learning_chain() - —É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
    print("  ‚Ä¢ decision_tree_chain() - –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π")

    print("\nüîß –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
    print("  ‚Ä¢ ai_manager.rag_simple.clear_cache() - –æ—á–∏—Å—Ç–∏—Ç—å Simple RAG –∫–µ—à")
    print("  ‚Ä¢ ai_manager.rag.rebuild_index() - –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å LangChain –∏–Ω–¥–µ–∫—Å")
    print("  ‚Ä¢ ai_manager.rag.get_cache_info() - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–µ—à–µ")

    print("\nüí° –°–æ–≤–µ—Ç—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:")
    print("  ‚Ä¢ –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É 'knowledge_base' —Å .txt —Ñ–∞–π–ª–∞–º–∏ –¥–ª—è RAG")
    print("  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ RAG –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞–º—è—Ç—å –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ –∏ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
    print("  ‚Ä¢ –¶–µ–ø–æ—á–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —Ç–µ–º—ã")
