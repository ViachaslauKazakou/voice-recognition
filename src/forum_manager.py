from typing import List, Dict, Optional, Union
import json
import re
from src.utils import timer, setup_logger
from src.rag_langchain import AdvancedRAG
from ollama import chat
from ollama import ChatResponse
from src.ai_manager import AIModels

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
logger = setup_logger(__name__)


class CharacterPersona:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏ —Ñ–æ—Ä—É–º–∞"""

    CHARACTERS = {
        "Alaev": {
            "type": "forum_troll",
            "personality": "35 –ª–µ—Ç, –Ø–∑–≤–∏—Ç–µ–ª—å–Ω—ã–π –∏ –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π, –≥—Ä—É–±—ã–π, –ª—é–±–∏—Ç —Å–ø–æ—Ä–∏—Ç—å, —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞—Ä–∫–∞–∑–º",
            "speech_pattern": "–î–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –æ—Ç–æ—Ä–≤–∞–Ω–Ω—ã–µ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –µ–∑–¥–∏—Ç –Ω–∞ –º–∞–º–∏–Ω–æ–π –í–æ–ª—å–≤–æ, –ª—é–±–∏—Ç –æ—Ç—Ä–∏—Ü–∞—Ç—å –≤—Å–µ –∏ —Å–ø–æ—Ä–∏—Ç—å —Å –æ–∫—Ä—É–∂–∞—é—â–∏–º–∏",
            "expertise": "–°—Ç–∞—Ä—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫—Ä–∏—Ç–∏–∫–∞ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤",
            "mood_variations": ["sarcastic", "aggressive", "nostalgic", "provocative"],
        },
        "Senior_Dev": {
            "type": "senior_python_engineer",
            "personality": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, —Ç–µ—Ä–ø–µ–ª–∏–≤—ã–π, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π",
            "speech_pattern": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è",
            "expertise": "Python, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, best practices",
            "mood_variations": ["helpful", "professional", "patient", "analytical"],
        },
        "Data_Scientist": {
            "type": "senior_data_scientist",
            "personality": "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π, –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –º–µ—Ç–æ–¥–∏—á–Ω—ã–π",
            "speech_pattern": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –≥—Ä–∞—Ñ–∏–∫–∏, –Ω–∞—É—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥",
            "expertise": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
            "mood_variations": ["analytical", "curious", "methodical", "research_focused"],
        },
        "Forum_Moderator": {
            "type": "forum_moderator",
            "personality": "–î–∏–ø–ª–æ–º–∞—Ç–∏—á–Ω—ã–π, —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—ã–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫",
            "speech_pattern": "–í–µ–∂–ª–∏–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –ø—Ä–∏–∑—ã–≤—ã –∫ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
            "expertise": "–ú–æ–¥–µ—Ä–∞—Ü–∏—è, —ç—Ç–∏–∫–∞, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º",
            "mood_variations": ["diplomatic", "firm", "encouraging", "neutral"],
        },
    }


class ForumRAG(AdvancedRAG):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π RAG –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–æ—Ä—É–º–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏"""

    def __init__(self, documents_path: str = "forum_knowledge_base", cache_path: str = "forum_cache"):
        super().__init__(documents_path, cache_path)
        self.character_persona = CharacterPersona()
        self.model = AIModels.gemma  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å Gemma3 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def parse_character_message(self, text: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏–∑ JSON –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
        try:
            # –ï—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞
            if text.strip().startswith("{") and text.strip().endswith("}"):
                data = json.loads(text)
                return self._normalize_json_message(data)

            # –ï—Å–ª–∏ —ç—Ç–æ –º–∞—Å—Å–∏–≤ JSON –æ–±—ä–µ–∫—Ç–æ–≤
            if text.strip().startswith("[") and text.strip().endswith("]"):
                data = json.loads(text)
                if isinstance(data, list) and len(data) > 0:
                    return self._normalize_json_message(data[0])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ

            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ JSON –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ–¥—Ä—è–¥
            json_objects = self._extract_json_objects(text)
            if json_objects:
                return self._normalize_json_message(json_objects[0])

        except json.JSONDecodeError:
            pass

        # Fallback –∫ –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        return self._parse_text_format(text)

    def _extract_json_objects(self, text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –æ–±—ä–µ–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        json_objects = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ JSON –æ–±—ä–µ–∫—Ç–æ–≤
        pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
        matches = re.finditer(pattern, text, re.DOTALL)

        for match in matches:
            try:
                json_obj = json.loads(match.group())
                json_objects.append(json_obj)
            except json.JSONDecodeError:
                continue

        return json_objects

    def _normalize_json_message(self, data: Dict) -> Dict:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç JSON —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON
        if "messages" in data and isinstance(data["messages"], list):
            # –§–æ—Ä–º–∞—Ç: {"messages": [{"character": "...", "content": "..."}]}
            message = data["messages"][0] if data["messages"] else {}
        else:
            # –§–æ—Ä–º–∞—Ç: {"character": "...", "content": "..."}
            message = data

        return {
            "character": message.get("character", "unknown"),
            "type": message.get("character_type", message.get("type", "unknown")),
            "mood": message.get("mood", "neutral"),
            "context": message.get("context", "general"),
            "content": message.get("content", message.get("message", "")),
            "timestamp": message.get("timestamp", ""),
            "reply_to": message.get("reply_to"),
            "id": message.get("id", ""),
            "raw_text": json.dumps(message, ensure_ascii=False),
        }

    def _parse_text_format(self, text: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç (fallback)"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        pattern = r"\[CHARACTER: ([^|]+) \| TYPE: ([^|]+) \| MOOD: ([^|]+) \| CONTEXT: ([^\]]+)\]"
        match = re.search(pattern, text)

        if match:
            character, char_type, mood, context = match.groups()
            content = text[match.end() :].strip()

            return {
                "character": character.strip(),
                "type": char_type.strip(),
                "mood": mood.strip(),
                "context": context.strip(),
                "content": content,
                "raw_text": text,
            }

        return {"content": text, "raw_text": text}

    def get_character_relevant_docs(self, query: str, character: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        if not self.vectorstore:
            logger.warning("Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []

        try:
            # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
            character_queries = [
                f'"{character}"',  # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–º–µ–Ω–∏
                f"character: {character}",  # –ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª—é character
                f"{character} {query}",  # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                query,  # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
            ]

            all_docs = []

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            for char_query in character_queries:
                try:
                    docs_with_scores = self.vectorstore.similarity_search_with_score(char_query, k=top_k)

                    for doc, score in docs_with_scores:
                        # –ü–∞—Ä—Å–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                        parsed = self.parse_character_message(doc.page_content)

                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂—É
                        if (
                            parsed.get("character") == character
                            or not parsed.get("character")
                            or parsed.get("character") == "unknown"
                        ):

                            similarity_score = 1.0 / (1.0 + score)

                            doc_info = {
                                "content": parsed.get("content", doc.page_content),
                                "character": parsed.get("character", "unknown"),
                                "type": parsed.get("type", "unknown"),
                                "mood": parsed.get("mood", "neutral"),
                                "context": parsed.get("context", "general"),
                                "timestamp": parsed.get("timestamp", ""),
                                "similarity_score": similarity_score,
                                "distance_score": float(score),
                                "metadata": doc.metadata,
                                "raw_text": doc.page_content,
                                "query_type": char_query,
                            }
                            all_docs.append(doc_info)

                except Exception as e:
                    logger.debug(f"Query '{char_query}' failed: {e}")
                    continue

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            unique_docs = {}
            for doc in all_docs:
                key = doc["content"][:100]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –∫–ª—é—á
                if key not in unique_docs or doc["similarity_score"] > unique_docs[key]["similarity_score"]:
                    unique_docs[key] = doc

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            character_docs = list(unique_docs.values())
            character_docs.sort(key=lambda x: x["similarity_score"], reverse=True)

            logger.info(f"Found {len(character_docs)} relevant documents for character {character}")
            return character_docs[:top_k]

        except Exception as e:
            logger.error(f"Error getting character documents: {e}")
            return []

    def get_character_context(self, character: str, mood: str = None) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        if character not in self.character_persona.CHARACTERS:
            logger.warning(f"Character {character} not found in persona")
            return ""

        char_info = self.character_persona.CHARACTERS[character]

        context = f"""
            –¢—ã –∏–≥—Ä–∞–µ—à—å —Ä–æ–ª—å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character} –Ω–∞ —Ñ–æ—Ä—É–º–µ.

            –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞:
            - –¢–∏–ø: {char_info['type']}
            - –õ–∏—á–Ω–æ—Å—Ç—å: {char_info['personality']}
            - –°—Ç–∏–ª—å —Ä–µ—á–∏: {char_info['speech_pattern']}
            - –≠–∫—Å–ø–µ—Ä—Ç–∏–∑–∞: {char_info['expertise']}
            - –¢–µ–∫—É—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood or '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ'}

            –û—Ç–≤–µ—á–∞–π –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ —ç—Ç–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞, –∏—Å–ø–æ–ª—å–∑—É—è –µ–≥–æ —Å—Ç–∏–ª—å —Ä–µ—á–∏ –∏ –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ–±–ª–µ–º–∞–º.
            """
        return context

    def validate_json_format(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å JSON —Ñ–æ—Ä–º–∞—Ç–∞ –≤ —Ñ–∞–π–ª–µ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å —Ñ–∞–π–ª –∫–∞–∫ JSON
            try:
                json.loads(content)
                return True
            except json.JSONDecodeError:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ JSON –æ–±—ä–µ–∫—Ç—ã
                json_objects = self._extract_json_objects(content)
                return len(json_objects) > 0

        except Exception as e:
            logger.error(f"Error validating JSON format: {e}")
            return False

    def convert_text_to_json(self, text_content: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ JSON"""
        messages = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
        pattern = r"\[CHARACTER: ([^|]+) \| TYPE: ([^|]+) \| MOOD: ([^|]+) \| CONTEXT: ([^\]]+)\]\s*([^[]*)"
        matches = re.finditer(pattern, text_content, re.MULTILINE | re.DOTALL)

        for i, match in enumerate(matches):
            character, char_type, mood, context, content = match.groups()

            message = {
                "id": f"msg_{i+1:03d}",
                "character": character.strip(),
                "character_type": char_type.strip(),
                "mood": mood.strip(),
                "context": context.strip(),
                "content": content.strip(),
                "timestamp": "",
                "reply_to": None,
            }
            messages.append(message)

        return json.dumps({"messages": messages}, ensure_ascii=False, indent=2)

    def get_character_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º –≤ –±–∞–∑–µ"""
        if not self.vectorstore:
            return {}

        stats = {}
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            all_docs = self.vectorstore.similarity_search("", k=1000)

            for doc in all_docs:
                parsed = self.parse_character_message(doc.page_content)
                character = parsed.get("character", "unknown")

                if character not in stats:
                    stats[character] = {"count": 0, "moods": set(), "contexts": set(), "types": set()}

                stats[character]["count"] += 1
                stats[character]["moods"].add(parsed.get("mood", "neutral"))
                stats[character]["contexts"].add(parsed.get("context", "general"))
                stats[character]["types"].add(parsed.get("type", "unknown"))

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON serialization
            for char in stats:
                stats[char]["moods"] = list(stats[char]["moods"])
                stats[char]["contexts"] = list(stats[char]["contexts"])
                stats[char]["types"] = list(stats[char]["types"])

        except Exception as e:
            logger.error(f"Error getting character stats: {e}")

        return stats

    @timer
    def simulate_forum_discussion(self, topic: str, participants: List[str] = None, rounds: int = 3):
        """–°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ñ–æ—Ä—É–º–Ω—É—é –¥–∏—Å–∫—É—Å—Å–∏—é –º–µ–∂–¥—É –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏"""
        if not participants:
            participants = ["Alaev", "Senior_Dev", "Data_Scientist", "Forum_Moderator"]

        logger.info(f"Starting forum discussion on: {topic}")

        discussion = []
        current_topic = topic

        for round_num in range(rounds):
            logger.info(f"Discussion round {round_num + 1}")

            for participant in participants:
                # –ö–∞–∂–¥—ã–π –ø–µ—Ä—Å–æ–Ω–∞–∂ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —Ç–µ–∫—É—â—É—é —Ç–µ–º—É
                response = self.ask_as_character(
                    f"–û–±—Å—É–∂–¥–∞–µ–º —Ç–µ–º—É: {current_topic}. –í—ã—Å–∫–∞–∂–∏ —Å–≤–æ–µ –º–Ω–µ–Ω–∏–µ.", participant, translate=True
                )

                discussion.append(
                    {"round": round_num + 1, "character": participant, "message": response, "topic": current_topic}
                )

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–º—É –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞
                current_topic = f"{topic}. –ü—Ä–µ–¥—ã–¥—É—â–∏–π —É—á–∞—Å—Ç–Ω–∏–∫ —Å–∫–∞–∑–∞–ª: {response[:100]}..."

        return discussion

    def get_available_characters(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π"""
        return list(self.character_persona.CHARACTERS.keys())

    def get_character_info(self, character: str) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ"""
        return self.character_persona.CHARACTERS.get(character, {})


class ForumManager:

    def __init__(self):
        # ...existing initialization...
        self.forum_rag = ForumRAG("forum_knowledge_base", "forum_cache")
        self.character_persona = CharacterPersona()
        self.model = AIModels.gemma 

    @timer
    def ask_as_character(self, prompt: str, character: str, mood: str = None, translate: bool = True):
        """–û—Ç–≤–µ—á–∞–µ—Ç –æ—Ç –∏–º–µ–Ω–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        logger.info(f"Asking as character {character} with mood {mood}: {prompt[:100]}...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂
        if character not in self.character_persona.CHARACTERS:
            logger.warning(f"Character {character} not found. Using default.")
            character = "Senior_Dev"

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        character_docs = self.forum_rag.get_character_relevant_docs(prompt, character)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        context = ""
        if character_docs:
            context = f"\n–ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π {character}:\n"
            for i, doc in enumerate(character_docs, 1):
                context += f"{i}. [{doc['mood']}] {doc['content'][:200]}...\n"
            context += "\n"

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        character_context = self.forum_rag.get_character_context(character, mood)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt = f"{prompt} (–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"

        full_prompt = f"""[INST]{character_context}

{context}

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: {prompt}

–û—Ç–≤–µ—Ç—å –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character}.
[/INST]"""

        try:
            response: ChatResponse = chat(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": full_prompt,
                    },
                ],
            )

            answer = response["message"]["content"]
            logger.info(f"Character {character} response generated")
            return answer

        except Exception as e:
            logger.error(f"Error generating character response: {e}")
            return f"[{character}] –ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å."

    def save_messages(self, character: str, response: str, question: Optional[str] = None, context: Optional[str] = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª answers.txt"""
        try:
            if not question:
                question = "–ë–µ–∑ –≤–æ–ø—Ä–æ—Å–∞"

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            message = {
                "character": character,
                "content": response,
                "context": context or "–û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç",
                "timestamp": "",
                "reply_to": None,
                "question": question,
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            with open("answers.txt", "a", encoding="utf-8") as f:
                f.write(json.dumps(message, ensure_ascii=False) + "\n")

            logger.info(f"Message from {character} saved successfully")
        except Exception as e:
            logger.error(f"Error saving message: {e}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    ai_manager = ForumManager()
    question = "–†–µ–∞–ª—å–Ω–æ –≤ 35 –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–≤–∞? –ß—Ç–æ –ª—É—á—à–µ, –º–µ—Ö–∞–Ω–∏–∫–∞ –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç?"
    # –ü—Ä–∏–º–µ—Ä 1: –û—Ç–≤–µ—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
    print("üé≠ –û—Ç–≤–µ—Ç –æ—Ç Alaev:")
    response = ai_manager.ask_as_character(
        question, 
        "Alaev", 
        mood="sarcastic"
    )
    print(f"Alaev: {response["result"]}")
    ai_manager.save_messages(
        "Alaev", 
        response, 
        # context="–û–±—Å—É–∂–¥–µ–Ω–∏–µ –≤–æ–¥–∏—Ç–µ√∑–ª–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∞—Ö"
        question=question
    )
    
    # print("\nüë®‚Äçüíª –û—Ç–≤–µ—Ç –æ—Ç Senior_Dev:")
    # senior_response = ai_manager.ask_as_character(
    #     "–ß—Ç–æ –¥—É–º–∞–µ—à—å –æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º Python?", 
    #     "Senior_Dev", 
    #     mood="professional"
    # )
    # print(f"Senior_Dev: {senior_response}")
    
    # # –ü—Ä–∏–º–µ—Ä 2: –°–∏–º—É–ª—è—Ü–∏—è —Ñ–æ—Ä—É–º–Ω–æ–π –¥–∏—Å–∫—É—Å—Å–∏–∏
    # print("\nüó£Ô∏è –°–∏–º—É–ª—è—Ü–∏—è —Ñ–æ—Ä—É–º–Ω–æ–π –¥–∏—Å–∫—É—Å—Å–∏–∏:")
    # discussion = ai_manager.simulate_forum_discussion(
    #     "–°—Ç–æ–∏—Ç –ª–∏ –∏–∑—É—á–∞—Ç—å Python –≤ 2024 –≥–æ–¥—É?",
    #     participants=["Alaev", "Senior_Dev", "Data_Scientist"],
    #     rounds=2
    # )
    
    # for msg in discussion:
    #     print(f"\n[–†–∞—É–Ω–¥ {msg['round']}] {msg['character']}: {msg['message'][:200]}...")